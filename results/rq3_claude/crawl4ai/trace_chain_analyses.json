[
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "quickstart.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult.media",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "quickstart.md -> ExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy.__init__()",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "quickstart.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "quickstart.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.metadata",
    "ground_truth_chain": "content-processing.md -> CrawlResult.metadata",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "cosine.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "content-processing.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "index.md -> AsyncWebCrawler",
    "ground_truth_chain": "index.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "index.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "index.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "index.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "session-management.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "page-interaction.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "quickstart.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "quickstart.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "overview.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "overview.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "overview.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "page-interaction.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "browser-config.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.__init__()",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy.__init__()",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "quickstart.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "quickstart.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy.__init__",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy.extract",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.metadata",
    "ground_truth_chain": "content-processing.md -> CrawlResult.metadata",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy.__init__()",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "content-processing.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "output-formats.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "cosine.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy.__init__",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy.extract",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "page-interaction.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy.extract",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "index.md -> AsyncWebCrawler",
    "ground_truth_chain": "index.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "index.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "index.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "index.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "browser-config.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "quickstart.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "overview.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "overview.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "overview.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy.__init__()",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "index.md -> AsyncWebCrawler",
    "ground_truth_chain": "index.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "index.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "index.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "index.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "quickstart.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "quickstart.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "quickstart.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "content-processing.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy.__init__",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy.semantic_filter",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.semantic_filter",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy.sim_threshold",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.sim_threshold",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy.word_count_threshold",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.word_count_threshold",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy.top_k",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.top_k",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy.__init__()",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "cosine.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy.__init__()",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "output-formats.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy.__init__",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy.extract",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.extract",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "page-interaction.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "overview.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "overview.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "overview.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "quickstart.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.remove_overlay_elements()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.remove_overlay_elements()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "overview.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.metadata",
    "ground_truth_chain": "content-processing.md -> CrawlResult.metadata",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "browser-config.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "overview.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "overview.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "overview.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "quickstart.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "content-processing.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.metadata",
    "ground_truth_chain": "content-processing.md -> CrawlResult.metadata",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "output-formats.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "cosine.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "index.md -> AsyncWebCrawler",
    "ground_truth_chain": "index.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "index.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "index.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "index.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult.cleaned_html",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "browser-config.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.__init__",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.word_count_threshold",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.word_count_threshold",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.top_k",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.top_k",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.semantic_filter",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.semantic_filter",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.sim_threshold",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.sim_threshold",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.max_dist",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.max_dist",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "quickstart.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "quickstart.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "overview.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "overview.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "overview.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "overview.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> LLMExtractionStrategy -> AsyncWebCrawler",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "browser-config.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "output-formats.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "quickstart.md -> ChunkingStrategy",
    "ground_truth_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_chain": "chunking.md -> RegexChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_chain": "chunking.md -> SlidingWindowChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "page-interaction.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "magic-mode.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "session-management.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "magic-mode.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-selection.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-selection.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "content-selection.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "content-selection.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "quickstart.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "cosine.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "cosine.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.metadata",
    "ground_truth_chain": "content-processing.md -> CrawlResult.metadata",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "browser-config.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "index.md -> AsyncWebCrawler",
    "ground_truth_chain": "index.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "index.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "index.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "index.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "index.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "content-processing.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css-advanced.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css-advanced.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "ground_truth_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "session-management.md -> AsyncWebCrawler",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "session-management.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "session-management.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "quickstart.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> CosineStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "page-interaction.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "page-interaction.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CosineStrategy",
    "ground_truth_chain": "overview.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "overview.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> CrawlResult",
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "output-formats.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
    "ground_truth_chain": "output-formats.md -> CrawlResult.fit_markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css-advanced.md -> ExtractionStrategy",
    "ground_truth_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "ground_truth_chain": "quickstart.md -> CrawlResult.screenshot",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "quickstart.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "cosine.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy.__init__()",
    "predicted_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__()",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "llm.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "output-formats.md -> LLMExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "output-formats.md -> LLMExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "output-formats.md -> LLMExtractionStrategy -> AsyncWebCrawler",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "output-formats.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "output-formats.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_chain": "chunking.md -> FixedLengthWordChunking",
    "ground_truth_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-processing.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "ground_truth_chain": "content-processing.md -> CrawlResult.cleaned_html",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "content-processing.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-processing.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "llm.md -> AsyncWebCrawler",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "llm.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "llm.md -> LLMExtractionStrategy",
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "ground_truth_chain": "llm.md -> CrawlResult.extracted_content",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "llm.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "proxy-security.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "cosine.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "cosine.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "content-processing.md -> CrawlResult",
    "ground_truth_chain": "content-processing.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "content-processing.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ChunkingStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
    "ground_truth_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "quickstart.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "quickstart.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "quickstart.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "quickstart.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "content-selection.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "content-selection.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "ground_truth_chain": null,
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.markdown",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.media",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.links",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "css.md -> AsyncWebCrawler",
    "ground_truth_chain": "css.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "PARTIAL_MATCH_PREFIX",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "css.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult",
    "predicted_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "css.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> CrawlResult.extracted_content",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> CrawlResult.success",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "css.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.success",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.error_message",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.error_message",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.status_code",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult.status_code",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Statement-level"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "overview.md -> ExtractionStrategy",
    "ground_truth_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "ground_truth_chain": "simple-crawling.md -> CrawlResult",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "ground_truth_chain": "browser-config.md -> AsyncWebCrawler.arun()",
    "interior_pattern": "EXTENDED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Method"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
    "interior_pattern": "SUBSTITUTED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> AsyncCrawlerStrategy",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_chain": null,
    "ground_truth_chain": "browser-config.md -> CrawlResult",
    "interior_pattern": "N/A",
    "correctness": "INCORRECT",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "interior_pattern": "N/A",
    "correctness": "COMPLETE_MATCH",
    "traceability_granularity": "Class"
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_chain": "css.md -> ExtractionStrategy",
    "ground_truth_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
    "interior_pattern": "SHORTENED",
    "correctness": "PARTIAL_MATCH_INTERMEDIATE",
    "traceability_granularity": "Class"
  }
]