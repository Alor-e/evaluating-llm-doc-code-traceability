{
  "dataset": "crawl4ai",
  "runs": [
    {
      "run_id": 1,
      "timestamp": "2025-02-12T10:57:35.436245",
      "negative_results": [
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "related",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly related as it would likely be the return type of the arun() method used in session-based crawling.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The document discusses using arun() for crawling, which would return a CrawlResult object containing the crawled data.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 18,
          "artifact_title": "CrawlResult.screenshot",
          "predicted_relationship": "implicit",
          "relationship_type": "populates",
          "relationship_explanation": "The 'screenshot=True' parameter in the arun() call suggests that the screenshot attribute of the CrawlResult will be populated.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
          "predicted_trace_chain_explanation": "The arun() method, when called with screenshot=True, populates the screenshot attribute of the returned CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "relates to",
          "relationship_explanation": "While not directly mentioned, the 'extracted_content' attribute relates to the concept of 'fit_markdown' discussed in the documentation. The documentation describes 'fit_markdown' as a feature for smart content extraction, which likely corresponds to the 'extracted_content' attribute in the code.",
          "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document discusses content extraction, which is likely implemented through the extracted_content attribute of CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "explicit",
          "relationship_type": "uses",
          "relationship_explanation": "The code explicitly accesses the 'extracted_content' attribute of the result object, which is likely a CrawlResult instance.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document shows the use of the extracted_content attribute, which is part of the CrawlResult class returned by arun().",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.fit_markdown",
          "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.fit_markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is likely used to store the results of the extraction process performed by LLMExtractionStrategy. This is inferred from the context of web crawling and data extraction.",
          "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> CrawlResult",
          "predicted_trace_chain_explanation": "The document describes using LLMExtractionStrategy, which typically produces results that would be stored in a CrawlResult object.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.__init__()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The __init__() method is implicitly used in the code example when creating an instance of JsonCssExtractionStrategy with a schema parameter: 'strategy = JsonCssExtractionStrategy(schema)'.",
          "predicted_trace_chain": "overview.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
          "predicted_trace_chain_explanation": "The documentation shows the creation of a JsonCssExtractionStrategy instance, which implicitly calls its __init__() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The documentation shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult object, leading to the implicit use of CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'html' attribute of CrawlResult is not directly used in the example, but it's an important part of the CrawlResult class which is implicitly used in the code.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The document shows usage of CrawlResult, which includes the html attribute, even though it's not directly accessed in this example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 25,
          "artifact_title": "NlpSentenceChunking.chunk()",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk() method is explicitly demonstrated in the usage example: 'chunks = chunker.chunk(text)'. This method implements the actual functionality of splitting text into sentences.",
          "predicted_trace_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
          "predicted_trace_chain_explanation": "The document describes the NlpSentenceChunking class, which extends ChunkingStrategy and implements the chunk() method. The chain shows the relationship from the documentation to the specific method implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "LLMExtractionStrategy",
          "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CosineStrategy",
          "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'markdown' attribute of CrawlResult is implicitly used in the code example when accessing 'result.markdown'.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "The document shows the usage chain from AsyncWebCrawler to arun() to CrawlResult, finally accessing the markdown attribute.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "While not explicitly mentioned, the JsonCssExtractionStrategy likely implements or extends ChunkingStrategy as it's a type of extraction strategy",
          "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "JsonCssExtractionStrategy is used in the document, and it likely implements or extends ChunkingStrategy as a base class for extraction strategies",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 27,
          "artifact_title": "TopicSegmentationChunking.__init__",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__ method is implicitly referenced through the description of the class parameter. The documentation states: 'Parameters: num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.' This indicates the presence of an initialization method with a configurable parameter.",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
          "predicted_trace_chain_explanation": "The document describes the TopicSegmentationChunking class, which necessarily includes its initialization method with the specified parameter.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 27,
          "artifact_title": "TopicSegmentationChunking.chunk",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk method is explicitly demonstrated in the usage example. The documentation shows: 'chunks = chunker.chunk(text)', which directly calls the chunk method of the TopicSegmentationChunking instance.",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
          "predicted_trace_chain_explanation": "The document describes the TopicSegmentationChunking class and explicitly demonstrates the use of its chunk method in the example code.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.__init__()",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__() method of CosineStrategy is implicitly demonstrated in the code examples, showing how to initialize the strategy with various parameters like word_count_threshold, top_k, semantic_filter, sim_threshold, max_dist, and verbose.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__()",
          "predicted_trace_chain_explanation": "The documentation shows how to initialize CosineStrategy with various parameters, implying the existence and usage of its __init__() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with content chunking and processing. The word_count_threshold parameter suggests that CosineStrategy implements chunking functionality.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The documentation describes CosineStrategy, which likely extends ChunkingStrategy to provide specific chunking behavior based on cosine similarity.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly called in the code snippet, the arun() method is implicitly related as it's the main method for performing crawls with AsyncWebCrawler. The comment '# We'll add our crawling code here' suggests that arun() would be called within the async context manager.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The documentation shows creation of AsyncWebCrawler, which would typically be followed by using its arun() method for crawling.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is likely used internally by JsonCssExtractionStrategy as it's a base class for extraction strategies.",
          "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The documentation uses JsonCssExtractionStrategy, which likely extends or uses ChunkingStrategy for its functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 2,
      "timestamp": "2025-02-12T14:59:02.644267",
      "negative_results": [
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the arun() method is implicitly related to this documentation. The code snippet creates an AsyncWebCrawler instance and mentions 'We'll add our crawling code here', which implies the use of methods like arun() for actual crawling operations.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The documentation shows how to set up AsyncWebCrawler, which is a prerequisite for using its methods like arun(). The arun() method would typically be used within the context manager to perform crawling operations.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 25,
          "artifact_title": "NlpSentenceChunking.chunk()",
          "predicted_relationship": "explicit",
          "relationship_type": "uses",
          "relationship_explanation": "The chunk() method of NlpSentenceChunking is explicitly used in the example provided in the documentation. This is evidenced by the line 'chunks = chunker.chunk(text)' in the example code.",
          "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
          "predicted_trace_chain_explanation": "The documentation describes the NlpSentenceChunking class, which includes the chunk() method. The example demonstrates the direct use of this method to split text into sentences.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy.__init__()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The __init__() method of LLMExtractionStrategy is implicitly used in the example code snippet. It's called when creating an instance of LLMExtractionStrategy with parameters like provider, api_token, and instruction.",
          "predicted_trace_chain": "llm.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
          "predicted_trace_chain_explanation": "The documentation shows how to create an instance of LLMExtractionStrategy, which implicitly calls its __init__() method to set up the strategy with the provided parameters.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.__init__",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The document provides a complex HTML structure example, suggesting that JsonCssExtractionStrategy would need to be initialized with a schema that matches this structure. This implies the use of the __init__ method to set up the extraction strategy with the appropriate CSS selectors for categories, products, reviews, etc.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__",
          "predicted_trace_chain_explanation": "The document discusses JsonCssExtractionStrategy, which would need to be initialized using its __init__ method to handle the complex HTML structure presented.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The document presents a complex HTML structure and discusses using JsonCssExtractionStrategy to extract data from it. This implies the use of the extract method to perform the actual data extraction based on the CSS selectors defined in the schema.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
          "predicted_trace_chain_explanation": "The document discusses using JsonCssExtractionStrategy for extraction, which would involve calling its extract method to process the HTML structure.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy.__init__()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The __init__() method is implicitly used when creating instances of LLMExtractionStrategy with different providers and API keys. This is evident from the usage examples provided in the documentation.",
          "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
          "predicted_trace_chain_explanation": "The document shows usage of LLMExtractionStrategy, which implicitly calls its __init__() method when instantiated.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "CosineStrategy is likely extending ChunkingStrategy as it's part of the extraction strategy hierarchy, though not explicitly mentioned in the text.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy is likely a subclass of ChunkingStrategy, which is the base class for extraction strategies.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The documentation provides examples of JSON structures for extracting nested objects, simple lists, nested lists, and lists of objects. These structures would be parsed and used within the extract() method of the JsonCssExtractionStrategy class to perform the actual extraction based on the specified selectors and types.",
          "predicted_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
          "predicted_trace_chain_explanation": "The document describes extraction structures that would be implemented in the extract() method of the JsonCssExtractionStrategy class, which extends the base ExtractionStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.__init__",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The schema provided in the documentation would be passed to the __init__ method of JsonCssExtractionStrategy to configure the extraction process.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__",
          "predicted_trace_chain_explanation": "The document provides a schema that would be used to initialize a JsonCssExtractionStrategy instance, implying the use of its __init__ method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The schema in the documentation defines the structure for extraction, which would be processed by the extract method of JsonCssExtractionStrategy to perform the actual content extraction.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
          "predicted_trace_chain_explanation": "The document describes the structure for extraction, which would be implemented in the extract method of JsonCssExtractionStrategy to perform the actual extraction process.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 18,
          "artifact_title": "CrawlResult.screenshot",
          "predicted_relationship": "implicit",
          "relationship_type": "stores",
          "relationship_explanation": "The 'screenshot=True' parameter in the arun() call suggests that a screenshot will be taken and stored in the CrawlResult.screenshot attribute.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
          "predicted_trace_chain_explanation": "The arun() method takes a screenshot which is then stored in the screenshot attribute of the CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "While not explicitly mentioned, the extract() method is implied by the description of how the strategy works. The documentation describes the schema and how data is extracted using CSS selectors, which would be implemented in the extract() method.",
          "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
          "predicted_trace_chain_explanation": "The document (css.md) describes the JsonCssExtractionStrategy, which would implement its functionality primarily through the extract() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "related",
          "relationship_explanation": "While not directly mentioned, the ChunkingStrategy is implicitly related as it is the base class for extraction strategies used in the examples.",
          "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The document uses extraction strategies, which are likely based on ChunkingStrategy. AsyncWebCrawler uses these strategies, forming a chain from the document to ChunkingStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the 'arun' method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The documentation shows the use of AsyncWebCrawler.arun(), which returns a CrawlResult object, allowing access to its properties.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text processing and chunking, which is a core functionality of chunking strategies.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The document discusses CosineStrategy, which is likely a specific implementation of the more general ChunkingStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 27,
          "artifact_title": "TopicSegmentationChunking.__init__",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__ method is implicitly described in the Parameters section, which mentions 'num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.' This indicates that the constructor takes this parameter.",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
          "predicted_trace_chain_explanation": "The document describes the initialization parameter for TopicSegmentationChunking, which is implemented in its __init__ method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 27,
          "artifact_title": "TopicSegmentationChunking.chunk",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk method is explicitly demonstrated in the example usage: 'chunks = chunker.chunk(text)'. This shows that the TopicSegmentationChunking class implements a chunk method that takes a text input and returns chunks.",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
          "predicted_trace_chain_explanation": "The document provides an example of using the chunk method of TopicSegmentationChunking, demonstrating its implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The extracted_content attribute of the CrawlResult class is implicitly used in the code example. The result of the arun() method is assumed to have an extracted_content attribute, which is then parsed as JSON.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The documentation implicitly uses the extracted_content attribute of the CrawlResult returned by the arun() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The document discusses extracting data using CSS selectors and JSON schemas, which is the primary function of the extract method. This is implied in statements like 'Use the `default` key in your field definitions to handle cases where data might be missing' and 'Use the `transform` key to clean or format extracted data'.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
          "predicted_trace_chain_explanation": "The document describes how to use JsonCssExtractionStrategy, which implies using its extract method to perform the actual data extraction based on the defined schema.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The markdown attribute of CrawlResult is implicitly used in the example. The code accesses the markdown property of the result returned by arun().",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "The document shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult object, from which the markdown attribute is accessed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "LLMExtractionStrategy",
          "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CosineStrategy",
          "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it's the return type of the arun() method, which is discussed in the context of session-based crawling.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The document discusses using arun() for session-based crawling, which implicitly involves the CrawlResult as the return type of arun().",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not directly accessed in the example, the html attribute is a core part of CrawlResult and would be populated by the arun() method.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The document shows the use of AsyncWebCrawler.arun(), which returns a CrawlResult, which contains the html attribute.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The extracted_content attribute is implicitly related to the fit_markdown feature described in the documentation, as it likely stores the extracted main content.",
          "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The documentation discusses fit_markdown, which is likely implemented or related to the extracted_content attribute of CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 3,
      "timestamp": "2025-02-12T15:10:41.014049",
      "negative_results": [
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy.__init__()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation shows usage of LLMExtractionStrategy with different providers and API keys, implying that these are likely parameters passed to the __init__() method when creating an instance of the class.",
          "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
          "predicted_trace_chain_explanation": "The documentation demonstrates creating instances of LLMExtractionStrategy with various parameters, which implies the use of its __init__() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text processing and clustering, which are core functionalities of chunking strategies.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy is described in the document, and it likely extends ChunkingStrategy to provide specific cosine similarity-based chunking functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The documentation provides examples of JSON structures for extracting data using CSS selectors. These structures would be processed by the extract() method of the JsonCssExtractionStrategy class to perform the actual extraction based on the provided schema.",
          "predicted_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
          "predicted_trace_chain_explanation": "The document describes the schema for extraction, which would be used by the JsonCssExtractionStrategy class. The extract() method of this class would implement the actual extraction logic based on the schema described in the documentation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with content chunking and filtering, which are core functionalities of chunking strategies.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy is described in the document, and it likely extends ChunkingStrategy to provide specific implementation for cosine similarity-based chunking.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.__init__",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__ method of CosineStrategy is implicitly shown in the usage examples, where parameters like semantic_filter, sim_threshold, word_count_threshold, and top_k are passed during instantiation.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
          "predicted_trace_chain_explanation": "The document shows how to create CosineStrategy instances with various parameters, which implies the existence and usage of the __init__ method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.semantic_filter",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The semantic_filter attribute is explicitly mentioned as a parameter that 'Sets the target topic or content type' and examples of its usage are provided.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.semantic_filter",
          "predicted_trace_chain_explanation": "The document directly describes the semantic_filter attribute of CosineStrategy and its purpose.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.sim_threshold",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The sim_threshold attribute is explicitly mentioned and described as controlling 'how similar content must be to be grouped together'. Usage examples with different values are provided.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.sim_threshold",
          "predicted_trace_chain_explanation": "The document directly describes the sim_threshold attribute of CosineStrategy, its purpose, and provides usage examples.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.word_count_threshold",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The word_count_threshold attribute is explicitly mentioned and described as filtering 'out short content blocks' to eliminate noise. A usage example is provided.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.word_count_threshold",
          "predicted_trace_chain_explanation": "The document directly describes the word_count_threshold attribute of CosineStrategy, its purpose, and provides a usage example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.top_k",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The top_k attribute is explicitly mentioned and described as the 'Number of top content clusters to return'. A usage example is provided.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.top_k",
          "predicted_trace_chain_explanation": "The document directly describes the top_k attribute of CosineStrategy, its purpose, and provides a usage example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.__init__()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation presents a complex HTML structure that would require initialization of the JsonCssExtractionStrategy with a specific schema. This implies the use of the __init__() method to set up the extraction strategy for the given HTML structure.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
          "predicted_trace_chain_explanation": "The document describes a complex HTML structure that would need to be defined in a schema, which is passed to the JsonCssExtractionStrategy through its initialization method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation provides a detailed HTML structure and mentions demonstrating 'various advanced features of JsonCssExtractionStrategy'. This implies the use of the extract() method to process the HTML and extract the structured data according to the defined schema.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
          "predicted_trace_chain_explanation": "The document presents an HTML structure that would be processed by the JsonCssExtractionStrategy's extract method to demonstrate its advanced features.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 25,
          "artifact_title": "NlpSentenceChunking.chunk()",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk() method is explicitly demonstrated in the usage example. The documentation shows: 'chunks = chunker.chunk(text)', which directly calls the chunk() method of the NlpSentenceChunking class.",
          "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
          "predicted_trace_chain_explanation": "The document describes NlpSentenceChunking, which includes the chunk() method as part of its interface. The chain shows the relationship from the class to its specific method implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.__init__()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The JsonCssExtractionStrategy constructor is implicitly used in the code example when creating an instance with 'JsonCssExtractionStrategy(schema)'. It initializes the strategy with the provided schema.",
          "predicted_trace_chain": "content-selection.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
          "predicted_trace_chain_explanation": "The document shows the creation of a JsonCssExtractionStrategy instance, which implicitly calls its constructor.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The extracted_content attribute of CrawlResult is implicitly used in the code example when accessing 'result.extracted_content' to parse the JSON content.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document shows the use of the extracted_content attribute to access the results of the extraction strategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not directly mentioned, the ChunkingStrategy is implicitly used as part of the CosineStrategy. The CosineStrategy likely employs a chunking strategy internally to process the text.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy, as an extraction strategy, typically involves text chunking, which is handled by a ChunkingStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it's the typical return type of the arun() method in AsyncWebCrawler, which is discussed in the context of session-based crawling.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The document discusses using arun() for session-based crawling, which typically returns a CrawlResult object.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it is a strategy for content extraction and chunking.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The use of CosineStrategy implies the extension of ChunkingStrategy, which is a base class for extraction strategies.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not directly accessed in the example, the html attribute is implicitly part of the CrawlResult object returned by arun().",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The document shows arun() being called, which returns a CrawlResult, which contains the html attribute.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.__init__",
          "predicted_relationship": "implicit",
          "relationship_type": "initializes",
          "relationship_explanation": "The document describes the Cosine Strategy's functionality, which implies the initialization of various components such as the semantic filter, word count threshold, and similarity calculations. These are likely set up in the __init__ method of the CosineStrategy class.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
          "predicted_trace_chain_explanation": "The document describes the Cosine Strategy, which is implemented by the CosineStrategy class. The initialization of this strategy's components would be handled in the __init__ method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.extract",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The document outlines the steps of how the Cosine Strategy works, including breaking down content, converting text to vectors, calculating similarity, clustering, and ranking. These steps are likely implemented in the extract method of the CosineStrategy class.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.extract",
          "predicted_trace_chain_explanation": "The document describes the process of the Cosine Strategy, which would be implemented in the extract method of the CosineStrategy class to perform the actual content extraction and clustering.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not directly mentioned, JsonCssExtractionStrategy likely extends or implements ChunkingStrategy, as it's used as an extraction strategy in the AsyncWebCrawler.",
          "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The documentation uses JsonCssExtractionStrategy, which is likely a type of ChunkingStrategy used for content extraction.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, the JsonCssExtractionStrategy likely extends or implements the ChunkingStrategy interface, as it's a type of extraction strategy.",
          "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The document uses JsonCssExtractionStrategy, which is likely an implementation of the more general ChunkingStrategy interface.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CosineStrategy",
          "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "LLMExtractionStrategy",
          "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not directly named, the extracted_content attribute (which fit_markdown is likely an alias or processed version of) is implicitly used when accessing 'result.fit_markdown' in the code examples.",
          "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document demonstrates accessing fit_markdown, which is likely related to or derived from the extracted_content attribute of CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 18,
          "artifact_title": "CrawlResult.screenshot",
          "predicted_relationship": "implicit",
          "relationship_type": "sets",
          "relationship_explanation": "The 'screenshot=True' parameter in the arun() call implicitly sets the screenshot attribute of the CrawlResult object.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
          "predicted_trace_chain_explanation": "The arun() method is called with screenshot=True, which leads to setting the screenshot attribute in the returned CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.remove_overlay_elements()",
          "predicted_relationship": "implicit",
          "relationship_type": "calls",
          "relationship_explanation": "The 'remove_overlay_elements=True' parameter in the arun() call implicitly triggers the remove_overlay_elements() method of AsyncPlaywrightCrawlerStrategy.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.remove_overlay_elements()",
          "predicted_trace_chain_explanation": "The arun() call with remove_overlay_elements=True leads to the invocation of the remove_overlay_elements() method in AsyncPlaywrightCrawlerStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 27,
          "artifact_title": "TopicSegmentationChunking.__init__",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__ method is implicitly referenced through the description of the 'num_keywords' parameter. The documentation states: 'Parameters: num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.'",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
          "predicted_trace_chain_explanation": "The document describes the initialization parameter for TopicSegmentationChunking, which directly relates to its __init__ method. This chain shows the logical flow from the document to the specific method implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 27,
          "artifact_title": "TopicSegmentationChunking.chunk",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk method is explicitly mentioned and demonstrated in the usage example. The documentation shows: 'chunks = chunker.chunk(text)'",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
          "predicted_trace_chain_explanation": "The document provides a usage example that directly calls the chunk method of TopicSegmentationChunking. This chain represents the flow from the document's example to the actual method being invoked.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CosineStrategy likely uses a ChunkingStrategy internally to divide the content into processable chunks before applying similarity-based clustering.",
          "predicted_trace_chain": "overview.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The CosineStrategy, as described in the document, performs content analysis and clustering. This process typically requires chunking the text, which is likely handled by a ChunkingStrategy implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is part of the CrawlResult class.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The arun() method returns a CrawlResult object, which is then used to access the extracted_content.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the AsyncWebCrawler.arun() method is implicitly related to this documentation. The code snippet creates an AsyncWebCrawler instance and mentions 'We'll add our crawling code here', which would typically involve using the arun() method.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The documentation shows creating an AsyncWebCrawler instance, which would typically be used to call the arun() method for crawling.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'markdown' attribute of CrawlResult is implicitly used in the code example when accessing 'result.markdown' to return the content.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "The document shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult, from which the markdown attribute is accessed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 4,
      "timestamp": "2025-02-12T15:18:09.754944",
      "negative_results": [
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "explicit",
          "relationship_type": "uses",
          "relationship_explanation": "The extracted_content attribute of CrawlResult is explicitly used in the code example. It's accessed to retrieve the JSON content extracted by the JsonCssExtractionStrategy.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document demonstrates the use of extracted_content, which is accessed through the CrawlResult object returned by arun().",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is likely the base class for ExtractionStrategy, which JsonCssExtractionStrategy implements. This relationship is inferred from the typical structure of extraction strategies in web crawling frameworks.",
          "predicted_trace_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The document uses JsonCssExtractionStrategy, which likely extends ExtractionStrategy, which in turn may extend ChunkingStrategy in the crawling framework hierarchy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it's the return type of the arun() method, which is discussed in the context of session-based crawling.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The arun() method, which is mentioned for session-based crawling, returns a CrawlResult. Hence, CrawlResult is implicitly used in this context.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CosineStrategy",
          "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "LLMExtractionStrategy",
          "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is implicitly used as it is the base class for extraction strategies. Both JsonCssExtractionStrategy and LLMExtractionStrategy likely extend or implement ChunkingStrategy.",
          "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The document uses extraction strategies, which are likely derived from ExtractionStrategy, which in turn is likely derived from ChunkingStrategy. This chain represents the inheritance hierarchy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The documentation shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult object, forming an implicit chain.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 28,
          "artifact_title": "TopicSegmentationChunking.__init__",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__ method is implicitly referenced through the description of the 'num_keywords' parameter. The text states: 'Parameters: num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.'",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
          "predicted_trace_chain_explanation": "The document describes a parameter for TopicSegmentationChunking, which implies the existence of an __init__ method to handle this parameter. The chain shows how this initialization is part of the class implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 28,
          "artifact_title": "TopicSegmentationChunking.chunk",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk method is explicitly mentioned and used in the example code. The text shows: 'chunks = chunker.chunk(text)'",
          "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
          "predicted_trace_chain_explanation": "The document provides an example of using the chunk method of TopicSegmentationChunking. The chain demonstrates how this method is a key part of the class's functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used. The line 'result = await crawler.arun(...)' suggests that the returned object (result) is likely an instance of CrawlResult, as it contains properties like 'extracted_content' which are accessed later.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The documentation uses AsyncWebCrawler.arun(), which returns a CrawlResult. This chain shows how the result of the crawling operation is represented.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The markdown attribute of CrawlResult is implicitly used in the example. It's accessed via result.markdown to retrieve the content of the crawled page.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "The document shows the usage of arun(), which returns a CrawlResult, from which the markdown attribute is accessed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 18,
          "artifact_title": "CrawlResult.screenshot",
          "predicted_relationship": "implicit",
          "relationship_type": "stores",
          "relationship_explanation": "The 'screenshot=True' parameter in the arun() call suggests that a screenshot will be taken and stored, likely in the CrawlResult.screenshot attribute.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
          "predicted_trace_chain_explanation": "The screenshot requested in the arun() call would be stored in the screenshot attribute of the CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.__init__",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__ method of CosineStrategy is implicitly referenced in the code examples where CosineStrategy is instantiated with various parameters. The documentation provides guidance on how to set these parameters, which are likely arguments to the __init__ method.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
          "predicted_trace_chain_explanation": "The documentation shows how to create and configure CosineStrategy instances, which directly involves calling the __init__ method with specific parameters.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.word_count_threshold",
          "predicted_relationship": "explicit",
          "relationship_type": "configures",
          "relationship_explanation": "The word_count_threshold attribute is explicitly mentioned in the code examples and best practices section. It's described as a parameter to filter content based on word count, with recommendations for different types of content.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.word_count_threshold",
          "predicted_trace_chain_explanation": "The documentation directly discusses and provides examples of setting the word_count_threshold attribute when initializing CosineStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.top_k",
          "predicted_relationship": "explicit",
          "relationship_type": "configures",
          "relationship_explanation": "The top_k attribute is explicitly mentioned in the code examples. It's described as a parameter to limit the number of results or relevant sections returned by the strategy.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.top_k",
          "predicted_trace_chain_explanation": "The documentation provides examples of setting the top_k attribute when initializing CosineStrategy, demonstrating its usage and importance in controlling output.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.semantic_filter",
          "predicted_relationship": "explicit",
          "relationship_type": "configures",
          "relationship_explanation": "The semantic_filter attribute is explicitly mentioned in the code example for handling different content types. It's used to focus the extraction on specific types of content, such as 'product features'.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.semantic_filter",
          "predicted_trace_chain_explanation": "The documentation shows how to use the semantic_filter attribute when initializing CosineStrategy for mixed content pages.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.sim_threshold",
          "predicted_relationship": "explicit",
          "relationship_type": "configures",
          "relationship_explanation": "The sim_threshold attribute is explicitly mentioned in the code example for handling different content types. It's described as a parameter for more flexible matching in mixed content scenarios.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.sim_threshold",
          "predicted_trace_chain_explanation": "The documentation provides an example of setting the sim_threshold attribute when initializing CosineStrategy for mixed content pages.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.max_dist",
          "predicted_relationship": "explicit",
          "relationship_type": "configures",
          "relationship_explanation": "The max_dist attribute is explicitly mentioned in the code example for handling different content types. It's described as a parameter for creating larger clusters in the cosine similarity-based clustering process.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.max_dist",
          "predicted_trace_chain_explanation": "The documentation shows how to set the max_dist attribute when initializing CosineStrategy for mixed content pages, illustrating its role in cluster formation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is likely used internally by CosineStrategy as it is a base class for chunking strategies, which are often used in conjunction with extraction strategies.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy likely uses a ChunkingStrategy internally for text processing.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation discusses the fit_markdown feature, which is likely related to or implemented as part of the extracted_content attribute of CrawlResult. While not explicitly named, it's implicitly used in the context of content extraction.",
          "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document discusses content extraction features which are likely implemented in the extracted_content attribute of CrawlResult, forming an implicit chain.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 25,
          "artifact_title": "NlpSentenceChunking.chunk()",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk() method is explicitly mentioned and demonstrated in the usage example. It is the core functionality of the NlpSentenceChunking class, used to split text into sentences. This is evidenced by the line 'chunks = chunker.chunk(text)' in the example code.",
          "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
          "predicted_trace_chain_explanation": "The document describes the NlpSentenceChunking class, which includes the chunk() method as its primary functionality for splitting text into sentences.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not directly accessed in the example, the html attribute is part of the CrawlResult class which is implicitly used in the code.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The document shows the usage of arun(), which returns a CrawlResult object, which contains the html attribute.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CosineStrategy likely uses a ChunkingStrategy internally to break down the content into manageable chunks before applying cosine similarity. This is implied by the use of 'word_count_threshold' parameter in the CosineStrategy initialization.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The document describes CosineStrategy, which likely uses ChunkingStrategy internally for text processing based on the parameters shown.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text processing and chunking, which is a core functionality described in the ChunkingStrategy abstract class.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy implements specific chunking behavior, which is likely based on the abstract ChunkingStrategy class.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "potentially uses",
          "relationship_explanation": "While not explicitly shown in the code snippet, the comment '# We'll add our crawling code here' suggests that the arun() method would likely be used within the async context manager. This is the primary method for performing crawls with AsyncWebCrawler.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The documentation shows the creation of an AsyncWebCrawler instance, which would typically be followed by calls to its arun() method for actual crawling operations.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        }
      ]
    },
    {
      "run_id": 5,
      "timestamp": "2025-02-12T15:25:19.578277",
      "negative_results": [
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "related",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly related as it would likely be the return type of the arun() method used in session-based crawling.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The document discusses using arun() for crawling, which would typically return a CrawlResult object containing the crawled data.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CosineStrategy",
          "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "LLMExtractionStrategy",
          "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 4,
          "artifact_title": "AsyncWebCrawler",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The AsyncWebCrawler class is implicitly related as it would likely be the class that utilizes the LLMExtractionStrategy for web crawling and data extraction. The async nature of the extract_structured_data_using_llm function suggests it's used in an asynchronous context, which aligns with AsyncWebCrawler.",
          "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> AsyncWebCrawler",
          "predicted_trace_chain_explanation": "The documentation shows usage of LLMExtractionStrategy, which would typically be used by AsyncWebCrawler for web data extraction tasks.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'markdown' attribute of CrawlResult is implicitly used in the code example when accessing 'result.markdown' to return the content.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "The document demonstrates AsyncWebCrawler usage, which calls arun(), returning a CrawlResult, from which the markdown attribute is accessed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_id": 9,
          "artifact_title": "CrawlResult.cleaned_html",
          "predicted_relationship": "implicit",
          "relationship_type": "potential use",
          "relationship_explanation": "While not directly used in this example, CrawlResult.cleaned_html is a potential output that could be accessed from the CrawlResult object returned by arun().",
          "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
          "predicted_trace_chain_explanation": "The document shows usage of arun(), which returns a CrawlResult object, which could potentially include cleaned_html.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "explicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'extracted_content' attribute of CrawlResult is explicitly used in the code snippet to access the extracted data: 'result.extracted_content'.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document shows the use of the extracted_content attribute from the CrawlResult returned by arun().",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 25,
          "artifact_title": "NlpSentenceChunking.chunk()",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The chunk() method is explicitly demonstrated in the usage example. The text shows: 'chunks = chunker.chunk(text)', which directly calls the chunk() method of the NlpSentenceChunking instance.",
          "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
          "predicted_trace_chain_explanation": "The document describes NlpSentenceChunking, which includes the chunk() method as part of its interface for splitting text into sentences.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "potentially uses",
          "relationship_explanation": "While not explicitly shown in the code snippet, the comment '# We'll add our crawling code here' implies that the arun() method would likely be used within the async context manager to perform the actual crawling operation.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The documentation shows the creation of an AsyncWebCrawler instance, which would then typically use its arun() method for crawling, though this is not explicitly shown in the snippet.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The extracted_content attribute is implicitly used through the fit_markdown property, which is described as a powerful feature for smart content extraction.",
          "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document discusses fit_markdown, which is likely implemented or related to the extracted_content attribute of CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core methods needed to implement the session management behaviors described in the documentation, including crawl() for executing page actions and hook management for handling session state.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "potential use",
          "relationship_explanation": "While not explicitly mentioned, the CosineStrategy is a type of extraction strategy that could potentially be used in place of the css_strategy or llm_strategy in the example.",
          "predicted_trace_chain": "overview.md -> AsyncWebCrawler.arun() -> CosineStrategy",
          "predicted_trace_chain_explanation": "The 'arun()' method accepts an extraction_strategy parameter, which could be an instance of CosineStrategy, though it's not explicitly shown in this example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy.extract()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The documentation provides detailed examples of JSON structures for CSS-based extraction. These structures would be processed by the extract() method of the JsonCssExtractionStrategy class. The method would interpret the nested objects, simple lists, nested lists, and lists of objects described in the documentation to perform the actual extraction.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
          "predicted_trace_chain_explanation": "The document describes the JSON structures for CSS extraction, which would be handled by the JsonCssExtractionStrategy class, specifically implemented in its extract() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, the CosineStrategy likely extends the ChunkingStrategy base class, as it is an extraction strategy that involves text chunking.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy, being an extraction strategy, likely extends the ChunkingStrategy base class for text processing functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text chunking and clustering, which are core functionalities of chunking strategies.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy is explicitly mentioned in the document, and it likely extends ChunkingStrategy to provide specific cosine similarity-based chunking functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 7,
          "artifact_title": "CosineStrategy.__init__()",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "The __init__() method of CosineStrategy is implicitly demonstrated in the usage examples, showing how to initialize the strategy with various parameters like word_count_threshold, top_k, semantic_filter, sim_threshold, and max_dist.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__()",
          "predicted_trace_chain_explanation": "The document shows how to initialize CosineStrategy with various parameters, which directly relates to its __init__() method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The documentation shows the use of AsyncWebCrawler.arun(), which returns a CrawlResult object, from which the extracted_content is accessed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with processing and chunking text content. The word_count_threshold parameter suggests that CosineStrategy implements text chunking functionality.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "CosineStrategy likely extends ChunkingStrategy to implement specific text chunking behavior based on cosine similarity.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'html' attribute of CrawlResult is not directly used in the example, but it's an important part of the CrawlResult class which is implicitly used.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "While not directly accessed, the html attribute is part of the CrawlResult returned by arun().",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    }
  ]
}