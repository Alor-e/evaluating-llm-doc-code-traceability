[
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "related",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly related as it would likely be the return type of the arun() method used in session-based crawling.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The document discusses using arun() for crawling, which would return a CrawlResult object containing the crawled data.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 18,
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "implicit",
    "relationship_type": "populates",
    "relationship_explanation": "The 'screenshot=True' parameter in the arun() call suggests that the screenshot attribute of the CrawlResult will be populated.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "predicted_trace_chain_explanation": "The arun() method, when called with screenshot=True, populates the screenshot attribute of the returned CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "screenshot: Optional[str] = None"
  },
  {
    "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "relates to",
    "relationship_explanation": "While not directly mentioned, the 'extracted_content' attribute relates to the concept of 'fit_markdown' discussed in the documentation. The documentation describes 'fit_markdown' as a feature for smart content extraction, which likely corresponds to the 'extracted_content' attribute in the code.",
    "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document discusses content extraction, which is likely implemented through the extracted_content attribute of CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "explicit",
    "relationship_type": "uses",
    "relationship_explanation": "The code explicitly accesses the 'extracted_content' attribute of the result object, which is likely a CrawlResult instance.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document shows the use of the extracted_content attribute, which is part of the CrawlResult class returned by arun().",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is likely used to store the results of the extraction process performed by LLMExtractionStrategy. This is inferred from the context of web crawling and data extraction.",
    "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> CrawlResult",
    "predicted_trace_chain_explanation": "The document describes using LLMExtractionStrategy, which typically produces results that would be stored in a CrawlResult object.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "document_location": "docs/md_v2/extraction/overview.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.__init__()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The __init__() method is implicitly used in the code example when creating an instance of JsonCssExtractionStrategy with a schema parameter: 'strategy = JsonCssExtractionStrategy(schema)'.",
    "predicted_trace_chain": "overview.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
    "predicted_trace_chain_explanation": "The documentation shows the creation of a JsonCssExtractionStrategy instance, which implicitly calls its __init__() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL", "IAE"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The documentation shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult object, leading to the implicit use of CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'html' attribute of CrawlResult is not directly used in the example, but it's an important part of the CrawlResult class which is implicitly used in the code.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The document shows usage of CrawlResult, which includes the html attribute, even though it's not directly accessed in this example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 25,
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk() method is explicitly demonstrated in the usage example: 'chunks = chunker.chunk(text)'. This method implements the actual functionality of splitting text into sentences.",
    "predicted_trace_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "predicted_trace_chain_explanation": "The document describes the NlpSentenceChunking class, which extends ChunkingStrategy and implements the chunk() method. The chain shows the relationship from the documentation to the specific method implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'markdown' attribute of CrawlResult is implicitly used in the code example when accessing 'result.markdown'.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "The document shows the usage chain from AsyncWebCrawler to arun() to CrawlResult, finally accessing the markdown attribute.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "While not explicitly mentioned, the JsonCssExtractionStrategy likely implements or extends ChunkingStrategy as it's a type of extraction strategy",
    "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "JsonCssExtractionStrategy is used in the document, and it likely implements or extends ChunkingStrategy as a base class for extraction strategies",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 27,
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__ method is implicitly referenced through the description of the class parameter. The documentation states: 'Parameters: num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.' This indicates the presence of an initialization method with a configurable parameter.",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "predicted_trace_chain_explanation": "The document describes the TopicSegmentationChunking class, which necessarily includes its initialization method with the specified parameter.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 27,
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk method is explicitly demonstrated in the usage example. The documentation shows: 'chunks = chunker.chunk(text)', which directly calls the chunk method of the TopicSegmentationChunking instance.",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "predicted_trace_chain_explanation": "The document describes the TopicSegmentationChunking class and explicitly demonstrates the use of its chunk method in the example code.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.__init__()",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__() method of CosineStrategy is implicitly demonstrated in the code examples, showing how to initialize the strategy with various parameters like word_count_threshold, top_k, semantic_filter, sim_threshold, max_dist, and verbose.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__()",
    "predicted_trace_chain_explanation": "The documentation shows how to initialize CosineStrategy with various parameters, implying the existence and usage of its __init__() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", 
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with content chunking and processing. The word_count_threshold parameter suggests that CosineStrategy implements chunking functionality.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The documentation describes CosineStrategy, which likely extends ChunkingStrategy to provide specific chunking behavior based on cosine similarity.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly called in the code snippet, the arun() method is implicitly related as it's the main method for performing crawls with AsyncWebCrawler. The comment '# We'll add our crawling code here' suggests that arun() would be called within the async context manager.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The documentation shows creation of AsyncWebCrawler, which would typically be followed by using its arun() method for crawling.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is likely used internally by JsonCssExtractionStrategy as it's a base class for extraction strategies.",
    "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The documentation uses JsonCssExtractionStrategy, which likely extends or uses ChunkingStrategy for its functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the arun() method is implicitly related to this documentation. The code snippet creates an AsyncWebCrawler instance and mentions 'We'll add our crawling code here', which implies the use of methods like arun() for actual crawling operations.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The documentation shows how to set up AsyncWebCrawler, which is a prerequisite for using its methods like arun(). The arun() method would typically be used within the context manager to perform crawling operations.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 25,
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_relationship": "explicit",
    "relationship_type": "uses",
    "relationship_explanation": "The chunk() method of NlpSentenceChunking is explicitly used in the example provided in the documentation. This is evidenced by the line 'chunks = chunker.chunk(text)' in the example code.",
    "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "predicted_trace_chain_explanation": "The documentation describes the NlpSentenceChunking class, which includes the chunk() method. The example demonstrates the direct use of this method to split text into sentences.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy.__init__()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The __init__() method of LLMExtractionStrategy is implicitly used in the example code snippet. It's called when creating an instance of LLMExtractionStrategy with parameters like provider, api_token, and instruction.",
    "predicted_trace_chain": "llm.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
    "predicted_trace_chain_explanation": "The documentation shows how to create an instance of LLMExtractionStrategy, which implicitly calls its __init__() method to set up the strategy with the provided parameters.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.__init__",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The document provides a complex HTML structure example, suggesting that JsonCssExtractionStrategy would need to be initialized with a schema that matches this structure. This implies the use of the __init__ method to set up the extraction strategy with the appropriate CSS selectors for categories, products, reviews, etc.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__",
    "predicted_trace_chain_explanation": "The document discusses JsonCssExtractionStrategy, which would need to be initialized using its __init__ method to handle the complex HTML structure presented.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The document presents a complex HTML structure and discusses using JsonCssExtractionStrategy to extract data from it. This implies the use of the extract method to perform the actual data extraction based on the CSS selectors defined in the schema.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
    "predicted_trace_chain_explanation": "The document discusses using JsonCssExtractionStrategy for extraction, which would involve calling its extract method to process the HTML structure.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy.__init__()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The __init__() method is implicitly used when creating instances of LLMExtractionStrategy with different providers and API keys. This is evident from the usage examples provided in the documentation.",
    "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
    "predicted_trace_chain_explanation": "The document shows usage of LLMExtractionStrategy, which implicitly calls its __init__() method when instantiated.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "CosineStrategy is likely extending ChunkingStrategy as it's part of the extraction strategy hierarchy, though not explicitly mentioned in the text.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy is likely a subclass of ChunkingStrategy, which is the base class for extraction strategies.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The documentation provides examples of JSON structures for extracting nested objects, simple lists, nested lists, and lists of objects. These structures would be parsed and used within the extract() method of the JsonCssExtractionStrategy class to perform the actual extraction based on the specified selectors and types.",
    "predicted_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "predicted_trace_chain_explanation": "The document describes extraction structures that would be implemented in the extract() method of the JsonCssExtractionStrategy class, which extends the base ExtractionStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.__init__",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The schema provided in the documentation would be passed to the __init__ method of JsonCssExtractionStrategy to configure the extraction process.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__",
    "predicted_trace_chain_explanation": "The document provides a schema that would be used to initialize a JsonCssExtractionStrategy instance, implying the use of its __init__ method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The schema in the documentation defines the structure for extraction, which would be processed by the extract method of JsonCssExtractionStrategy to perform the actual content extraction.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
    "predicted_trace_chain_explanation": "The document describes the structure for extraction, which would be implemented in the extract method of JsonCssExtractionStrategy to perform the actual extraction process.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 18,
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "implicit",
    "relationship_type": "stores",
    "relationship_explanation": "The 'screenshot=True' parameter in the arun() call suggests that a screenshot will be taken and stored in the CrawlResult.screenshot attribute.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "predicted_trace_chain_explanation": "The arun() method takes a screenshot which is then stored in the screenshot attribute of the CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", "IOL"
    ],
    "artifact_code": "screenshot: Optional[str] = None"
  },
  {
    "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "While not explicitly mentioned, the extract() method is implied by the description of how the strategy works. The documentation describes the schema and how data is extracted using CSS selectors, which would be implemented in the extract() method.",
    "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "predicted_trace_chain_explanation": "The document (css.md) describes the JsonCssExtractionStrategy, which would implement its functionality primarily through the extract() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "document_location": "docs/md_v2/basic/page-interaction.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "related",
    "relationship_explanation": "While not directly mentioned, the ChunkingStrategy is implicitly related as it is the base class for extraction strategies used in the examples.",
    "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The document uses extraction strategies, which are likely based on ChunkingStrategy. AsyncWebCrawler uses these strategies, forming a chain from the document to ChunkingStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the 'arun' method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The documentation shows the use of AsyncWebCrawler.arun(), which returns a CrawlResult object, allowing access to its properties.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text processing and chunking, which is a core functionality of chunking strategies.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The document discusses CosineStrategy, which is likely a specific implementation of the more general ChunkingStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 27,
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__ method is implicitly described in the Parameters section, which mentions 'num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.' This indicates that the constructor takes this parameter.",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "predicted_trace_chain_explanation": "The document describes the initialization parameter for TopicSegmentationChunking, which is implemented in its __init__ method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 27,
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk method is explicitly demonstrated in the example usage: 'chunks = chunker.chunk(text)'. This shows that the TopicSegmentationChunking class implements a chunk method that takes a text input and returns chunks.",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "predicted_trace_chain_explanation": "The document provides an example of using the chunk method of TopicSegmentationChunking, demonstrating its implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The extracted_content attribute of the CrawlResult class is implicitly used in the code example. The result of the arun() method is assumed to have an extracted_content attribute, which is then parsed as JSON.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The documentation implicitly uses the extracted_content attribute of the CrawlResult returned by the arun() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The document discusses extracting data using CSS selectors and JSON schemas, which is the primary function of the extract method. This is implied in statements like 'Use the `default` key in your field definitions to handle cases where data might be missing' and 'Use the `transform` key to clean or format extracted data'.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract",
    "predicted_trace_chain_explanation": "The document describes how to use JsonCssExtractionStrategy, which implies using its extract method to perform the actual data extraction based on the defined schema.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The markdown attribute of CrawlResult is implicitly used in the example. The code accesses the markdown property of the result returned by arun().",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "The document shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult object, from which the markdown attribute is accessed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it's the return type of the arun() method, which is discussed in the context of session-based crawling.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The document discusses using arun() for session-based crawling, which implicitly involves the CrawlResult as the return type of arun().",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not directly accessed in the example, the html attribute is a core part of CrawlResult and would be populated by the arun() method.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The document shows the use of AsyncWebCrawler.arun(), which returns a CrawlResult, which contains the html attribute.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The extracted_content attribute is implicitly related to the fit_markdown feature described in the documentation, as it likely stores the extracted main content.",
    "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The documentation discusses fit_markdown, which is likely implemented or related to the extracted_content attribute of CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy.__init__()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation shows usage of LLMExtractionStrategy with different providers and API keys, implying that these are likely parameters passed to the __init__() method when creating an instance of the class.",
    "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> LLMExtractionStrategy.__init__()",
    "predicted_trace_chain_explanation": "The documentation demonstrates creating instances of LLMExtractionStrategy with various parameters, which implies the use of its __init__() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text processing and clustering, which are core functionalities of chunking strategies.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy is described in the document, and it likely extends ChunkingStrategy to provide specific cosine similarity-based chunking functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The documentation provides examples of JSON structures for extracting data using CSS selectors. These structures would be processed by the extract() method of the JsonCssExtractionStrategy class to perform the actual extraction based on the provided schema.",
    "predicted_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "predicted_trace_chain_explanation": "The document describes the schema for extraction, which would be used by the JsonCssExtractionStrategy class. The extract() method of this class would implement the actual extraction logic based on the schema described in the documentation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with content chunking and filtering, which are core functionalities of chunking strategies.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy is described in the document, and it likely extends ChunkingStrategy to provide specific implementation for cosine similarity-based chunking.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.__init__",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__ method of CosineStrategy is implicitly shown in the usage examples, where parameters like semantic_filter, sim_threshold, word_count_threshold, and top_k are passed during instantiation.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
    "predicted_trace_chain_explanation": "The document shows how to create CosineStrategy instances with various parameters, which implies the existence and usage of the __init__ method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.semantic_filter",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The semantic_filter attribute is explicitly mentioned as a parameter that 'Sets the target topic or content type' and examples of its usage are provided.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.semantic_filter",
    "predicted_trace_chain_explanation": "The document directly describes the semantic_filter attribute of CosineStrategy and its purpose.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.sim_threshold",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The sim_threshold attribute is explicitly mentioned and described as controlling 'how similar content must be to be grouped together'. Usage examples with different values are provided.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.sim_threshold",
    "predicted_trace_chain_explanation": "The document directly describes the sim_threshold attribute of CosineStrategy, its purpose, and provides usage examples.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.word_count_threshold",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The word_count_threshold attribute is explicitly mentioned and described as filtering 'out short content blocks' to eliminate noise. A usage example is provided.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.word_count_threshold",
    "predicted_trace_chain_explanation": "The document directly describes the word_count_threshold attribute of CosineStrategy, its purpose, and provides a usage example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.top_k",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The top_k attribute is explicitly mentioned and described as the 'Number of top content clusters to return'. A usage example is provided.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.top_k",
    "predicted_trace_chain_explanation": "The document directly describes the top_k attribute of CosineStrategy, its purpose, and provides a usage example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.__init__()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation presents a complex HTML structure that would require initialization of the JsonCssExtractionStrategy with a specific schema. This implies the use of the __init__() method to set up the extraction strategy for the given HTML structure.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
    "predicted_trace_chain_explanation": "The document describes a complex HTML structure that would need to be defined in a schema, which is passed to the JsonCssExtractionStrategy through its initialization method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL",
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation provides a detailed HTML structure and mentions demonstrating 'various advanced features of JsonCssExtractionStrategy'. This implies the use of the extract() method to process the HTML and extract the structured data according to the defined schema.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "predicted_trace_chain_explanation": "The document presents an HTML structure that would be processed by the JsonCssExtractionStrategy's extract method to demonstrate its advanced features.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 25,
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk() method is explicitly demonstrated in the usage example. The documentation shows: 'chunks = chunker.chunk(text)', which directly calls the chunk() method of the NlpSentenceChunking class.",
    "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "predicted_trace_chain_explanation": "The document describes NlpSentenceChunking, which includes the chunk() method as part of its interface. The chain shows the relationship from the class to its specific method implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.__init__()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The JsonCssExtractionStrategy constructor is implicitly used in the code example when creating an instance with 'JsonCssExtractionStrategy(schema)'. It initializes the strategy with the provided schema.",
    "predicted_trace_chain": "content-selection.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.__init__()",
    "predicted_trace_chain_explanation": "The document shows the creation of a JsonCssExtractionStrategy instance, which implicitly calls its constructor.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The extracted_content attribute of CrawlResult is implicitly used in the code example when accessing 'result.extracted_content' to parse the JSON content.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document shows the use of the extracted_content attribute to access the results of the extraction strategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not directly mentioned, the ChunkingStrategy is implicitly used as part of the CosineStrategy. The CosineStrategy likely employs a chunking strategy internally to process the text.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy, as an extraction strategy, typically involves text chunking, which is handled by a ChunkingStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it's the typical return type of the arun() method in AsyncWebCrawler, which is discussed in the context of session-based crawling.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The document discusses using arun() for session-based crawling, which typically returns a CrawlResult object.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it is a strategy for content extraction and chunking.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The use of CosineStrategy implies the extension of ChunkingStrategy, which is a base class for extraction strategies.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not directly accessed in the example, the html attribute is implicitly part of the CrawlResult object returned by arun().",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The document shows arun() being called, which returns a CrawlResult, which contains the html attribute.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.__init__",
    "predicted_relationship": "implicit",
    "relationship_type": "initializes",
    "relationship_explanation": "The document describes the Cosine Strategy's functionality, which implies the initialization of various components such as the semantic filter, word count threshold, and similarity calculations. These are likely set up in the __init__ method of the CosineStrategy class.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
    "predicted_trace_chain_explanation": "The document describes the Cosine Strategy, which is implemented by the CosineStrategy class. The initialization of this strategy's components would be handled in the __init__ method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.extract",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The document outlines the steps of how the Cosine Strategy works, including breaking down content, converting text to vectors, calculating similarity, clustering, and ranking. These steps are likely implemented in the extract method of the CosineStrategy class.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.extract",
    "predicted_trace_chain_explanation": "The document describes the process of the Cosine Strategy, which would be implemented in the extract method of the CosineStrategy class to perform the actual content extraction and clustering.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not directly mentioned, JsonCssExtractionStrategy likely extends or implements ChunkingStrategy, as it's used as an extraction strategy in the AsyncWebCrawler.",
    "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The documentation uses JsonCssExtractionStrategy, which is likely a type of ChunkingStrategy used for content extraction.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, the JsonCssExtractionStrategy likely extends or implements the ChunkingStrategy interface, as it's a type of extraction strategy.",
    "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The document uses JsonCssExtractionStrategy, which is likely an implementation of the more general ChunkingStrategy interface.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not directly named, the extracted_content attribute (which fit_markdown is likely an alias or processed version of) is implicitly used when accessing 'result.fit_markdown' in the code examples.",
    "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document demonstrates accessing fit_markdown, which is likely related to or derived from the extracted_content attribute of CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 18,
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "implicit",
    "relationship_type": "sets",
    "relationship_explanation": "The 'screenshot=True' parameter in the arun() call implicitly sets the screenshot attribute of the CrawlResult object.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "predicted_trace_chain_explanation": "The arun() method is called with screenshot=True, which leads to setting the screenshot attribute in the returned CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", "IOL"
    ],
    "artifact_code": "screenshot: Optional[str] = None"
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.remove_overlay_elements()",
    "predicted_relationship": "implicit",
    "relationship_type": "calls",
    "relationship_explanation": "The 'remove_overlay_elements=True' parameter in the arun() call implicitly triggers the remove_overlay_elements() method of AsyncPlaywrightCrawlerStrategy.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.remove_overlay_elements()",
    "predicted_trace_chain_explanation": "The arun() call with remove_overlay_elements=True leads to the invocation of the remove_overlay_elements() method in AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL",
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 27,
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__ method is implicitly referenced through the description of the 'num_keywords' parameter. The documentation states: 'Parameters: num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.'",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "predicted_trace_chain_explanation": "The document describes the initialization parameter for TopicSegmentationChunking, which directly relates to its __init__ method. This chain shows the logical flow from the document to the specific method implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 27,
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk method is explicitly mentioned and demonstrated in the usage example. The documentation shows: 'chunks = chunker.chunk(text)'",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "predicted_trace_chain_explanation": "The document provides a usage example that directly calls the chunk method of TopicSegmentationChunking. This chain represents the flow from the document's example to the actual method being invoked.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "document_location": "docs/md_v2/extraction/overview.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CosineStrategy likely uses a ChunkingStrategy internally to divide the content into processable chunks before applying similarity-based clustering.",
    "predicted_trace_chain": "overview.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The CosineStrategy, as described in the document, performs content analysis and clustering. This process typically requires chunking the text, which is likely handled by a ChunkingStrategy implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is part of the CrawlResult class.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The arun() method returns a CrawlResult object, which is then used to access the extracted_content.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the AsyncWebCrawler.arun() method is implicitly related to this documentation. The code snippet creates an AsyncWebCrawler instance and mentions 'We'll add our crawling code here', which would typically involve using the arun() method.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The documentation shows creating an AsyncWebCrawler instance, which would typically be used to call the arun() method for crawling.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'markdown' attribute of CrawlResult is implicitly used in the code example when accessing 'result.markdown' to return the content.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "The document shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult, from which the markdown attribute is accessed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "explicit",
    "relationship_type": "uses",
    "relationship_explanation": "The extracted_content attribute of CrawlResult is explicitly used in the code example. It's accessed to retrieve the JSON content extracted by the JsonCssExtractionStrategy.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document demonstrates the use of extracted_content, which is accessed through the CrawlResult object returned by arun().",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is likely the base class for ExtractionStrategy, which JsonCssExtractionStrategy implements. This relationship is inferred from the typical structure of extraction strategies in web crawling frameworks.",
    "predicted_trace_chain": "content-selection.md -> JsonCssExtractionStrategy -> ExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The document uses JsonCssExtractionStrategy, which likely extends ExtractionStrategy, which in turn may extend ChunkingStrategy in the crawling framework hierarchy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it's the return type of the arun() method, which is discussed in the context of session-based crawling.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The arun() method, which is mentioned for session-based crawling, returns a CrawlResult. Hence, CrawlResult is implicitly used in this context.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "document_location": "docs/md_v2/basic/page-interaction.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is implicitly used as it is the base class for extraction strategies. Both JsonCssExtractionStrategy and LLMExtractionStrategy likely extend or implement ChunkingStrategy.",
    "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> ExtractionStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The document uses extraction strategies, which are likely derived from ExtractionStrategy, which in turn is likely derived from ChunkingStrategy. This chain represents the inheritance hierarchy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The documentation shows the usage of AsyncWebCrawler.arun(), which returns a CrawlResult object, forming an implicit chain.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 28,
    "artifact_title": "TopicSegmentationChunking.__init__",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__ method is implicitly referenced through the description of the 'num_keywords' parameter. The text states: 'Parameters: num_keywords (int, optional): Number of keywords for each topic segment. Default is 3.'",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.__init__",
    "predicted_trace_chain_explanation": "The document describes a parameter for TopicSegmentationChunking, which implies the existence of an __init__ method to handle this parameter. The chain shows how this initialization is part of the class implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 28,
    "artifact_title": "TopicSegmentationChunking.chunk",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk method is explicitly mentioned and used in the example code. The text shows: 'chunks = chunker.chunk(text)'",
    "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking -> TopicSegmentationChunking.chunk",
    "predicted_trace_chain_explanation": "The document provides an example of using the chunk method of TopicSegmentationChunking. The chain demonstrates how this method is a key part of the class's functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used. The line 'result = await crawler.arun(...)' suggests that the returned object (result) is likely an instance of CrawlResult, as it contains properties like 'extracted_content' which are accessed later.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The documentation uses AsyncWebCrawler.arun(), which returns a CrawlResult. This chain shows how the result of the crawling operation is represented.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The markdown attribute of CrawlResult is implicitly used in the example. It's accessed via result.markdown to retrieve the content of the crawled page.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "The document shows the usage of arun(), which returns a CrawlResult, from which the markdown attribute is accessed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 18,
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "implicit",
    "relationship_type": "stores",
    "relationship_explanation": "The 'screenshot=True' parameter in the arun() call suggests that a screenshot will be taken and stored, likely in the CrawlResult.screenshot attribute.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.screenshot",
    "predicted_trace_chain_explanation": "The screenshot requested in the arun() call would be stored in the screenshot attribute of the CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", "IOL"
    ],
    "artifact_code": "screenshot: Optional[str] = None"
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.__init__",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__ method of CosineStrategy is implicitly referenced in the code examples where CosineStrategy is instantiated with various parameters. The documentation provides guidance on how to set these parameters, which are likely arguments to the __init__ method.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__",
    "predicted_trace_chain_explanation": "The documentation shows how to create and configure CosineStrategy instances, which directly involves calling the __init__ method with specific parameters.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.word_count_threshold",
    "predicted_relationship": "explicit",
    "relationship_type": "configures",
    "relationship_explanation": "The word_count_threshold attribute is explicitly mentioned in the code examples and best practices section. It's described as a parameter to filter content based on word count, with recommendations for different types of content.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.word_count_threshold",
    "predicted_trace_chain_explanation": "The documentation directly discusses and provides examples of setting the word_count_threshold attribute when initializing CosineStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.top_k",
    "predicted_relationship": "explicit",
    "relationship_type": "configures",
    "relationship_explanation": "The top_k attribute is explicitly mentioned in the code examples. It's described as a parameter to limit the number of results or relevant sections returned by the strategy.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.top_k",
    "predicted_trace_chain_explanation": "The documentation provides examples of setting the top_k attribute when initializing CosineStrategy, demonstrating its usage and importance in controlling output.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL", "IOL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.semantic_filter",
    "predicted_relationship": "explicit",
    "relationship_type": "configures",
    "relationship_explanation": "The semantic_filter attribute is explicitly mentioned in the code example for handling different content types. It's used to focus the extraction on specific types of content, such as 'product features'.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.semantic_filter",
    "predicted_trace_chain_explanation": "The documentation shows how to use the semantic_filter attribute when initializing CosineStrategy for mixed content pages.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.sim_threshold",
    "predicted_relationship": "explicit",
    "relationship_type": "configures",
    "relationship_explanation": "The sim_threshold attribute is explicitly mentioned in the code example for handling different content types. It's described as a parameter for more flexible matching in mixed content scenarios.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.sim_threshold",
    "predicted_trace_chain_explanation": "The documentation provides an example of setting the sim_threshold attribute when initializing CosineStrategy for mixed content pages.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.max_dist",
    "predicted_relationship": "explicit",
    "relationship_type": "configures",
    "relationship_explanation": "The max_dist attribute is explicitly mentioned in the code example for handling different content types. It's described as a parameter for creating larger clusters in the cosine similarity-based clustering process.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.max_dist",
    "predicted_trace_chain_explanation": "The documentation shows how to set the max_dist attribute when initializing CosineStrategy for mixed content pages, illustrating its role in cluster formation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": ["PAL", "IOL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, ChunkingStrategy is likely used internally by CosineStrategy as it is a base class for chunking strategies, which are often used in conjunction with extraction strategies.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy likely uses a ChunkingStrategy internally for text processing.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation discusses the fit_markdown feature, which is likely related to or implemented as part of the extracted_content attribute of CrawlResult. While not explicitly named, it's implicitly used in the context of content extraction.",
    "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document discusses content extraction features which are likely implemented in the extracted_content attribute of CrawlResult, forming an implicit chain.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 25,
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk() method is explicitly mentioned and demonstrated in the usage example. It is the core functionality of the NlpSentenceChunking class, used to split text into sentences. This is evidenced by the line 'chunks = chunker.chunk(text)' in the example code.",
    "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "predicted_trace_chain_explanation": "The document describes the NlpSentenceChunking class, which includes the chunk() method as its primary functionality for splitting text into sentences.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not directly accessed in the example, the html attribute is part of the CrawlResult class which is implicitly used in the code.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The document shows the usage of arun(), which returns a CrawlResult object, which contains the html attribute.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CosineStrategy likely uses a ChunkingStrategy internally to break down the content into manageable chunks before applying cosine similarity. This is implied by the use of 'word_count_threshold' parameter in the CosineStrategy initialization.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The document describes CosineStrategy, which likely uses ChunkingStrategy internally for text processing based on the parameters shown.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text processing and chunking, which is a core functionality described in the ChunkingStrategy abstract class.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy implements specific chunking behavior, which is likely based on the abstract ChunkingStrategy class.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "potentially uses",
    "relationship_explanation": "While not explicitly shown in the code snippet, the comment '# We'll add our crawling code here' suggests that the arun() method would likely be used within the async context manager. This is the primary method for performing crawls with AsyncWebCrawler.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The documentation shows the creation of an AsyncWebCrawler instance, which would typically be followed by calls to its arun() method for actual crawling operations.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "related",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly related as it would likely be the return type of the arun() method used in session-based crawling.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The document discusses using arun() for crawling, which would typically return a CrawlResult object containing the crawled data.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 4,
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The AsyncWebCrawler class is implicitly related as it would likely be the class that utilizes the LLMExtractionStrategy for web crawling and data extraction. The async nature of the extract_structured_data_using_llm function suggests it's used in an asynchronous context, which aligns with AsyncWebCrawler.",
    "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy -> AsyncWebCrawler",
    "predicted_trace_chain_explanation": "The documentation shows usage of LLMExtractionStrategy, which would typically be used by AsyncWebCrawler for web data extraction tasks.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'markdown' attribute of CrawlResult is implicitly used in the code example when accessing 'result.markdown' to return the content.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "The document demonstrates AsyncWebCrawler usage, which calls arun(), returning a CrawlResult, from which the markdown attribute is accessed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "document_location": "docs/md_v2/basic/output-formats.md",
    "artifact_id": 9,
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "implicit",
    "relationship_type": "potential use",
    "relationship_explanation": "While not directly used in this example, CrawlResult.cleaned_html is a potential output that could be accessed from the CrawlResult object returned by arun().",
    "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
    "predicted_trace_chain_explanation": "The document shows usage of arun(), which returns a CrawlResult object, which could potentially include cleaned_html.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "cleaned_html: Optional[str] = None"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "explicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'extracted_content' attribute of CrawlResult is explicitly used in the code snippet to access the extracted data: 'result.extracted_content'.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document shows the use of the extracted_content attribute from the CrawlResult returned by arun().",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 25,
    "artifact_title": "NlpSentenceChunking.chunk()",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The chunk() method is explicitly demonstrated in the usage example. The text shows: 'chunks = chunker.chunk(text)', which directly calls the chunk() method of the NlpSentenceChunking instance.",
    "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> NlpSentenceChunking.chunk()",
    "predicted_trace_chain_explanation": "The document describes NlpSentenceChunking, which includes the chunk() method as part of its interface for splitting text into sentences.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "potentially uses",
    "relationship_explanation": "While not explicitly shown in the code snippet, the comment '# We'll add our crawling code here' implies that the arun() method would likely be used within the async context manager to perform the actual crawling operation.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The documentation shows the creation of an AsyncWebCrawler instance, which would then typically use its arun() method for crawling, though this is not explicitly shown in the snippet.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The extracted_content attribute is implicitly used through the fit_markdown property, which is described as a powerful feature for smart content extraction.",
    "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document discusses fit_markdown, which is likely implemented or related to the extracted_content attribute of CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "document_location": "docs/md_v2/extraction/overview.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "potential use",
    "relationship_explanation": "While not explicitly mentioned, the CosineStrategy is a type of extraction strategy that could potentially be used in place of the css_strategy or llm_strategy in the example.",
    "predicted_trace_chain": "overview.md -> AsyncWebCrawler.arun() -> CosineStrategy",
    "predicted_trace_chain_explanation": "The 'arun()' method accepts an extraction_strategy parameter, which could be an instance of CosineStrategy, though it's not explicitly shown in this example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)"
  },
  {
    "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy.extract()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The documentation provides detailed examples of JSON structures for CSS-based extraction. These structures would be processed by the extract() method of the JsonCssExtractionStrategy class. The method would interpret the nested objects, simple lists, nested lists, and lists of objects described in the documentation to perform the actual extraction.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> JsonCssExtractionStrategy.extract()",
    "predicted_trace_chain_explanation": "The document describes the JSON structures for CSS extraction, which would be handled by the JsonCssExtractionStrategy class, specifically implemented in its extract() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, the CosineStrategy likely extends the ChunkingStrategy base class, as it is an extraction strategy that involves text chunking.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy, being an extraction strategy, likely extends the ChunkingStrategy base class for text processing functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with text chunking and clustering, which are core functionalities of chunking strategies.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy is explicitly mentioned in the document, and it likely extends ChunkingStrategy to provide specific cosine similarity-based chunking functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 7,
    "artifact_title": "CosineStrategy.__init__()",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "The __init__() method of CosineStrategy is implicitly demonstrated in the usage examples, showing how to initialize the strategy with various parameters like word_count_threshold, top_k, semantic_filter, sim_threshold, and max_dist.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> CosineStrategy.__init__()",
    "predicted_trace_chain_explanation": "The document shows how to initialize CosineStrategy with various parameters, which directly relates to its __init__() method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "While not explicitly mentioned, the CrawlResult class is implicitly used as it is the return type of the arun() method. The code accesses the 'extracted_content' attribute of the result, which is a property of CrawlResult.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The documentation shows the use of AsyncWebCrawler.arun(), which returns a CrawlResult object, from which the extracted_content is accessed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly mentioned, CosineStrategy likely extends ChunkingStrategy as it deals with processing and chunking text content. The word_count_threshold parameter suggests that CosineStrategy implements text chunking functionality.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "CosineStrategy likely extends ChunkingStrategy to implement specific text chunking behavior based on cosine similarity.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'html' attribute of CrawlResult is not directly used in the example, but it's an important part of the CrawlResult class which is implicitly used.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "While not directly accessed, the html attribute is part of the CrawlResult returned by arun().",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  }
]