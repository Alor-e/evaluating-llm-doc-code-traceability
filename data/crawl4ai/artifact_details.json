[
  {
    "artifact_title": "AsyncCrawlerStrategy",
    "artifact_location": "crawl4ai/async_crawler_strategy.py",
    "artifact_content": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "artifact_location": "crawl4ai/async_crawler_strategy.py",
    "artifact_content": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "artifact_location": "crawl4ai/async_crawler_strategy.py",
    "artifact_content": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]",
    "artifact_type": "Method",
    "traceability_granularity": "Method"
  },
  {
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "artifact_location": "crawl4ai/async_crawler_strategy.py",
    "artifact_content": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")",
    "artifact_type": "Method",
    "traceability_granularity": "Method"
  },
  {
    "artifact_title": "AsyncWebCrawler",
    "artifact_location": "crawl4ai/async_webcrawler.py",
    "artifact_content": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "AsyncWebCrawler.arun()",
    "artifact_location": "crawl4ai/async_webcrawler.py",
    "artifact_content": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) ",
    "artifact_type": "Method",
    "traceability_granularity": "Method"
  },
  {
    "artifact_title": "ChunkingStrategy",
    "artifact_location": "crawl4ai/chunking_strategy.py",
    "artifact_content": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "CosineStrategy",
    "artifact_location": "crawl4ai/extraction_strategy.py",
    "artifact_content": "class CosineStrategy(ExtractionStrategy): def __init__(self, semantic_filter = None, word_count_threshold=10, max_dist=0.2, linkage_method='ward', top_k=3, model_name = 'sentence-transformers/all-MiniLM-L6-v2', sim_threshold = 0.3, **kwargs): \"\"\" Initialize the strategy with clustering parameters. Args: semantic_filter (str): A keyword filter for document filtering. word_count_threshold (int): Minimum number of words per cluster. max_dist (float): The maximum cophenetic distance on the dendrogram to form clusters. linkage_method (str): The linkage method for hierarchical clustering. top_k (int): Number of top categories to extract. \"\"\" super().__init__() import numpy as np self.semantic_filter = semantic_filter self.word_count_threshold = word_count_threshold self.max_dist = max_dist self.linkage_method = linkage_method self.top_k = top_k self.sim_threshold = sim_threshold self.timer = time.time() self.verbose = kwargs.get(\"verbose\", False) self.buffer_embeddings = np.array([]) self.get_embedding_method = \"direct\" self.device = get_device() # import torch # self.device = torch.device('cpu') self.default_batch_size = calculate_batch_size(self.device) if self.verbose: print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\") # if False and self.device.type == \"cpu\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" # else: self.tokenizer, self.model = load_HF_embedding_model(model_name) self.model.to(self.device) self.model.eval() self.get_embedding_method = \"batch\" self.buffer_embeddings = np.array([]) # if model_name == \"bert-base-uncased\": # self.tokenizer, self.model = load_bert_base_uncased() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"BAAI/bge-small-en-v1.5\": # self.tokenizer, self.model = load_bge_small_en_v1_5() # self.model.eval() # Ensure the model is in evaluation mode # self.get_embedding_method = \"batch\" # elif model_name == \"sentence-transformers/all-MiniLM-L6-v2\": # self.model = load_onnx_all_MiniLM_l6_v2() # self.tokenizer = self.model.tokenizer # self.get_embedding_method = \"direct\" if self.verbose: print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\") self.nlp, _ = load_text_multilabel_classifier() # self.default_batch_size = 16 if self.device.type == 'cpu' else 64 if self.verbose: print(f\"[LOG] Model loaded {model_name}, models/reuters, took \" + str(time.time() - self.timer) + \" seconds\") def filter_documents_embeddings(self, documents: List[str], semantic_filter: str, at_least_k: int = 20) -> List[str]: \"\"\" Filter and sort documents based on the cosine similarity of their embeddings with the semantic_filter embedding. :param documents: List of text chunks (documents). :param semantic_filter: A string containing the keywords for filtering. :param threshold: Cosine similarity threshold for filtering documents. :param at_least_k: Minimum number of documents to return. :return: List of filtered documents, ensuring at least `at_least_k` documents. \"\"\" if not semantic_filter: return documents if len(documents) < at_least_k: at_least_k = len(documents) // 2 from sklearn.metrics.pairwise import cosine_similarity # Compute embedding for the keyword filter query_embedding = self.get_embeddings([semantic_filter])[0] # Compute embeddings for the documents document_embeddings = self.get_embeddings(documents) # Calculate cosine similarity between the query embedding and document embeddings similarities = cosine_similarity([query_embedding], document_embeddings).flatten() # Filter documents based on the similarity threshold filtered_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim >= self.sim_threshold] # If the number of filtered documents is less than at_least_k, sort remaining documents by similarity if len(filtered_docs) < at_least_k: remaining_docs = [(doc, sim) for doc, sim in zip(documents, similarities) if sim < self.sim_threshold] remaining_docs.sort(key=lambda x: x[1], reverse=True) filtered_docs.extend(remaining_docs[:at_least_k - len(filtered_docs)]) # Extract the document texts from the tuples filtered_docs = [doc for doc, _ in filtered_docs] return filtered_docs[:at_least_k] def get_embeddings(self, sentences: List[str], batch_size=None, bypass_buffer=False): \"\"\" Get BERT embeddings for a list of sentences. :param sentences: List of text chunks (sentences). :return: NumPy array of embeddings. \"\"\" # if self.buffer_embeddings.any() and not bypass_buffer: # return self.buffer_embeddings if self.device.type in [ \"cpu\", \"gpu\", \"cuda\", \"mps\"]: import torch # Tokenize sentences and convert to tensor if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] encoded_input = self.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt') encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()} # Ensure no gradients are calculated with torch.no_grad(): model_output = self.model(**encoded_input) # Get embeddings from the last hidden state (mean pooling) embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy() all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) elif self.device.type == \"cpu\": # self.buffer_embeddings = self.model(sentences) if batch_size is None: batch_size = self.default_batch_size all_embeddings = [] for i in range(0, len(sentences), batch_size): batch_sentences = sentences[i:i + batch_size] embeddings = self.model(batch_sentences) all_embeddings.append(embeddings) self.buffer_embeddings = np.vstack(all_embeddings) return self.buffer_embeddings def hierarchical_clustering(self, sentences: List[str], embeddings = None): \"\"\" Perform hierarchical clustering on sentences and return cluster labels. :param sentences: List of text chunks (sentences). :return: NumPy array of cluster labels. \"\"\" # Get embeddings from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist self.timer = time.time() embeddings = self.get_embeddings(sentences, bypass_buffer=True) # print(f\"[LOG]  Embeddings computed in {time.time() - self.timer:.2f} seconds\") # Compute pairwise cosine distances distance_matrix = pdist(embeddings, 'cosine') # Perform agglomerative clustering respecting order linked = linkage(distance_matrix, method=self.linkage_method) # Form flat clusters labels = fcluster(linked, self.max_dist, criterion='distance') return labels def filter_clusters_by_word_count(self, clusters: Dict[int, List[str]]): \"\"\" Filter clusters to remove those with a word count below the threshold. :param clusters: Dictionary of clusters. :return: Filtered dictionary of clusters. \"\"\" filtered_clusters = {} for cluster_id, texts in clusters.items(): # Concatenate texts for analysis full_text = \" \".join(texts) # Count words word_count = len(full_text.split()) # Keep clusters with word count above the threshold if word_count >= self.word_count_threshold: filtered_clusters[cluster_id] = texts return filtered_clusters def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract clusters from HTML content using hierarchical clustering. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of dictionaries representing the clusters. \"\"\" # Assume `html` is a list of text chunks for this strategy t = time.time() text_chunks = html.split(self.DEL) # Split by lines or paragraphs as needed # Pre-filter documents using embeddings and semantic_filter text_chunks = self.filter_documents_embeddings(text_chunks, self.semantic_filter) if not text_chunks: return [] # Perform clustering labels = self.hierarchical_clustering(text_chunks) # print(f\"[LOG]  Clustering done in {time.time() - t:.2f} seconds\") # Organize texts by their cluster labels, retaining order t = time.time() clusters = {} for index, label in enumerate(labels): clusters.setdefault(label, []).append(text_chunks[index]) # Filter clusters by word count filtered_clusters = self.filter_clusters_by_word_count(clusters) # Convert filtered clusters to a sorted list of dictionaries cluster_list = [{\"index\": int(idx), \"tags\" : [], \"content\": \" \".join(filtered_clusters[idx])} for idx in sorted(filtered_clusters)] if self.verbose: print(f\"[LOG]  Assign tags using {self.device}\") if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]: labels = self.nlp([cluster['content'] for cluster in cluster_list]) for cluster, label in zip(cluster_list, labels): cluster['tags'] = label # elif self.device.type == \"cpu\": # # Process the text with the loaded model # texts = [cluster['content'] for cluster in cluster_list] # # Batch process texts # docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"]) # for doc, cluster in zip(docs, cluster_list): # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] # for cluster in cluster_list: # doc = self.nlp(cluster['content']) # tok_k = self.top_k # top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k] # cluster['tags'] = [cat for cat, _ in top_categories] if self.verbose: print(f\"[LOG]  Categorization done in {time.time() - t:.2f} seconds\") return cluster_list def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections using hierarchical clustering. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :param provider: The provider to be used for extraction (not used here). :param api_token: Optional API token for the provider (not used here). :return: A list of processed JSON blocks. \"\"\" # This strategy processes all sections together return self.extract(url, self.DEL.join(sections), **kwargs)",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "CrawlResult",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "CrawlResult.cleaned_html",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "cleaned_html: Optional[str] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.error_message",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "error_message: Optional[str] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.extracted_content",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "extracted_content: Optional[str] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.fit_markdown",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "fit_markdown: Optional[str] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.html",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "html: str",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.links",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "links: Dict[str, List[Dict]] = {}",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.markdown",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "markdown: Optional[str] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.media",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "media: Dict[str, List[Dict]] = {}",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.metadata",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "metadata: Optional[dict] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.screenshot",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "screenshot: Optional[str] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.status_code",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "status_code: Optional[int] = None",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "CrawlResult.success",
    "artifact_location": "crawl4ai/models.py",
    "artifact_content": "success: bool",
    "artifact_type": "Class Attribute",
    "traceability_granularity": "Statement-level"
  },
  {
    "artifact_title": "ExtractionStrategy",
    "artifact_location": "crawl4ai/extraction_strategy.py",
    "artifact_content": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content ",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "FixedLengthWordChunking",
    "artifact_location": "crawl4ai/chunking_strategy.py",
    "artifact_content": "class FixedLengthWordChunking(ChunkingStrategy): def __init__(self, chunk_size=100, **kwargs): \"\"\" Initialize the fixed-length word chunking strategy with the given chunk size. Args: chunk_size (int): The size of each chunk in words. \"\"\" self.chunk_size = chunk_size def chunk(self, text: str) -> list: words = text.split() return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "JsonCssExtractionStrategy",
    "artifact_location": "crawl4ai/extraction_strategy.py",
    "artifact_content": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "LLMExtractionStrategy",
    "artifact_location": "crawl4ai/extraction_strategy.py",
    "artifact_content": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "NlpSentenceChunking",
    "artifact_location": "crawl4ai/chunking_strategy.py",
    "artifact_content": "class NlpSentenceChunking(ChunkingStrategy): def __init__(self, **kwargs): load_nltk_punkt() pass def chunk(self, text: str) -> list: # Improved regex for sentence splitting # sentence_endings = re.compile( # r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z][A-Z]\\.)(?<![A-Za-z]\\.)(?<=\\.|\\?|\\!|\n)\\s' # ) # sentences = sentence_endings.split(text) # sens = [sent.strip() for sent in sentences if sent] from nltk.tokenize import sent_tokenize sentences = sent_tokenize(text) sens = [sent.strip() for sent in sentences] return list(set(sens))",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "RegexChunking",
    "artifact_location": "crawl4ai/chunking_strategy.py",
    "artifact_content": "class RegexChunking(ChunkingStrategy): def __init__(self, patterns=None, **kwargs): if patterns is None: patterns = [r'\n\n'] # Default split pattern self.patterns = patterns def chunk(self, text: str) -> list: paragraphs = [text] for pattern in self.patterns: new_paragraphs = [] for paragraph in paragraphs: new_paragraphs.extend(re.split(pattern, paragraph)) paragraphs = new_paragraphs return paragraphs",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "SlidingWindowChunking",
    "artifact_location": "crawl4ai/chunking_strategy.py",
    "artifact_content": "class SlidingWindowChunking(ChunkingStrategy): def __init__(self, window_size=100, step=50, **kwargs): \"\"\" Initialize the sliding window chunking strategy with the given window size and step size. Args: window_size (int): The size of the sliding window in words. step (int): The step size for sliding the window in words. \"\"\" self.window_size = window_size self.step = step def chunk(self, text: str) -> list: words = text.split() chunks = [] if len(words) <= self.window_size: return [text] for i in range(0, len(words) - self.window_size + 1, self.step): chunk = ' '.join(words[i:i + self.window_size]) chunks.append(chunk) # Handle the last chunk if it doesn't align perfectly if i + self.window_size < len(words): chunks.append(' '.join(words[-self.window_size:])) return chunks",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  },
  {
    "artifact_title": "TopicSegmentationChunking",
    "artifact_location": "crawl4ai/chunking_strategy.py",
    "artifact_content": "class TopicSegmentationChunking(ChunkingStrategy): def __init__(self, num_keywords=3, **kwargs): import nltk as nl self.tokenizer = nl.tokenize.TextTilingTokenizer() self.num_keywords = num_keywords def chunk(self, text: str) -> list: # Use the TextTilingTokenizer to segment the text segmented_topics = self.tokenizer.tokenize(text) return segmented_topics def extract_keywords(self, text: str) -> list: # Tokenize and remove stopwords and punctuation import nltk as nl tokens = nl.toknize.word_tokenize(text) tokens = [token.lower() for token in tokens if token not in nl.corpus.stopwords.words('english') and token not in string.punctuation] # Calculate frequency distribution freq_dist = Counter(tokens) keywords = [word for word, freq in freq_dist.most_common(self.num_keywords)] return keywords def chunk_with_topics(self, text: str) -> list: # Segment the text into topics segments = self.chunk(text) # Extract keywords for each topic segment segments_with_topics = [(segment, self.extract_keywords(segment)) for segment in segments] return segments_with_topics",
    "artifact_type": "Class",
    "traceability_granularity": "Class"
  }
]